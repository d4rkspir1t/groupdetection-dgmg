{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Stochastic Training of GNN for Link Prediction\n",
    "==============================================\n",
    "\n",
    "This tutorial will show how to train a multi-layer GraphSAGE for link\n",
    "prediction on ``ogbn-arxiv`` provided by `Open Graph Benchmark\n",
    "(OGB) <https://ogb.stanford.edu/>`__. The dataset\n",
    "contains around 170 thousand nodes and 1 million edges.\n",
    "\n",
    "By the end of this tutorial, you will be able to\n",
    "\n",
    "-  Train a GNN model for link prediction on a single GPU with DGL's\n",
    "   neighbor sampling components.\n",
    "\n",
    "This tutorial assumes that you have read the :doc:`Introduction of Neighbor\n",
    "Sampling for GNN Training <L0_neighbor_sampling_overview>` and :doc:`Neighbor\n",
    "Sampling for Node Classification <L1_large_node_classification>`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Prediction Overview\n",
    "------------------------\n",
    "\n",
    "Link prediction requires the model to predict the probability of\n",
    "existence of an edge. This tutorial does so by computing a dot product\n",
    "between the representations of both incident nodes.\n",
    "\n",
    "\\begin{align}\\hat{y}_{u\\sim v} = \\sigma(h_u^T h_v)\\end{align}\n",
    "\n",
    "It then minimizes the following binary cross entropy loss.\n",
    "\n",
    "\\begin{align}\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\\end{align}\n",
    "\n",
    "This is identical to the link prediction formulation in :doc:`the previous\n",
    "tutorial on link prediction <4_link_predict>`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Dataset\n",
    "---------------\n",
    "\n",
    "This tutorial loads the dataset from the ``ogb`` package as in the\n",
    ":doc:`previous tutorial <L1_large_node_classification>`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=169343, num_edges=2332486,\n",
      "      ndata_schemes={'year': Scheme(shape=(1,), dtype=torch.int64), 'feat': Scheme(shape=(128,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "tensor([[ 4],\n",
      "        [ 5],\n",
      "        [28],\n",
      "        ...,\n",
      "        [10],\n",
      "        [ 4],\n",
      "        [ 1]])\n",
      "Number of classes: 40\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-arxiv')\n",
    "device = 'cpu'      # change to 'cuda' for GPU\n",
    "\n",
    "graph, node_labels = dataset[0]\n",
    "# Add reverse edges since ogbn-arxiv is unidirectional.\n",
    "graph = dgl.add_reverse_edges(graph)\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "node_labels = node_labels[:, 0]\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Neighbor Sampler and Data Loader in DGL\n",
    "------------------------------------------------\n",
    "\n",
    "Different from the :doc:`link prediction tutorial for full\n",
    "graph <4_link_predict>`, a common practice to train GNN on large graphs is\n",
    "to iterate over the edges\n",
    "in minibatches, since computing the probability of all edges is usually\n",
    "impossible. For each minibatch of edges, you compute the output\n",
    "representation of their incident nodes using neighbor sampling and GNN,\n",
    "in a similar fashion introduced in the :doc:`large-scale node classification\n",
    "tutorial <L1_large_node_classification>`.\n",
    "\n",
    "DGL provides ``dgl.dataloading.EdgeDataLoader`` to\n",
    "iterate over edges for edge classification or link prediction tasks.\n",
    "\n",
    "To perform link prediction, you need to specify a negative sampler. DGL\n",
    "provides builtin negative samplers such as\n",
    "``dgl.dataloading.negative_sampler.Uniform``.  Here this tutorial uniformly\n",
    "draws 5 negative examples per positive example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sampler = dgl.dataloading.negative_sampler.Uniform(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the negative sampler, one can then define the edge data\n",
    "loader with neighbor sampling.  To create an ``EdgeDataLoader`` for\n",
    "link prediction, provide a neighbor sampler object as well as the negative\n",
    "sampler object created above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "train_dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "    # The following arguments are specific to NodeDataLoader.\n",
    "    graph,                                  # The graph\n",
    "    torch.arange(graph.number_of_edges()),  # The edges to iterate over\n",
    "    sampler,                                # The neighbor sampler\n",
    "    negative_sampler=negative_sampler,      # The negative sampler\n",
    "#     device='GPU',                          # Put the bipartite graphs on CPU or GPU\n",
    "    # The following arguments are inherited from PyTorch DataLoader.\n",
    "    batch_size=1024,    # Batch size\n",
    "    shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
    "    drop_last=False,    # Whether to drop the last incomplete batch\n",
    "    num_workers=0       # Number of sampler processes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peek one minibatch from ``train_dataloader`` and see what it\n",
    "will give you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input nodes: 56635\n",
      "Positive graph # nodes: 6885 # edges: 1024\n",
      "Negative graph # nodes: 6885 # edges: 5120\n",
      "[Block(num_src_nodes=56635, num_dst_nodes=23730, num_edges=88282), Block(num_src_nodes=23730, num_dst_nodes=6885, num_edges=23990)]\n"
     ]
    }
   ],
   "source": [
    "input_nodes, pos_graph, neg_graph, bipartites = next(iter(train_dataloader))\n",
    "print('Number of input nodes:', len(input_nodes))\n",
    "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
    "print('Negative graph # nodes:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\n",
    "print(bipartites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example minibatch consists of four elements.\n",
    "\n",
    "The first element is an ID tensor for the input nodes, i.e., nodes\n",
    "whose input features are needed on the first GNN layer for this minibatch.\n",
    "\n",
    "The second element and the third element are the positive graph and the\n",
    "negative graph for this minibatch.\n",
    "The concept of positive and negative graphs have been introduced in the\n",
    ":doc:`full-graph link prediction tutorial <4_link_predict>`.  In minibatch\n",
    "training, the positive graph and the negative graph only contain nodes\n",
    "necessary for computing the pair-wise scores of positive and negative examples\n",
    "in the current minibatch.\n",
    "\n",
    "The last element is a list of bipartite graphs storing the computation\n",
    "dependencies for each GNN layer.\n",
    "The bipartite graphs are used to compute the GNN outputs of the nodes\n",
    "involved in positive/negative graph.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model for Node Representation\n",
    "--------------------------------------\n",
    "\n",
    "The model is almost identical to the one in the :doc:`node classification\n",
    "tutorial <L1_large_node_classification>`. The only difference is\n",
    "that since you are doing link prediction, the output dimension will not\n",
    "be the number of classes in the dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "\n",
    "    def forward(self, bipartites, x):\n",
    "        h_dst = x[:bipartites[0].num_dst_nodes()]\n",
    "        h = self.conv1(bipartites[0], (x, h_dst))\n",
    "        h = F.relu(h)\n",
    "        h_dst = h[:bipartites[1].num_dst_nodes()]\n",
    "        h = self.conv2(bipartites[1], (h, h_dst))\n",
    "        return h\n",
    "\n",
    "model = Model(num_features, 128).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the Score Predictor for Edges\n",
    "--------------------------------------\n",
    "\n",
    "After getting the node representation necessary for the minibatch, the\n",
    "last thing to do is to predict the score of the edges and non-existent\n",
    "edges in the sampled minibatch.\n",
    "\n",
    "The following score predictor, copied from the :doc:`link prediction\n",
    "tutorial <4_link_predict>`, takes a dot product between the\n",
    "incident nodes’ representations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Performance (Optional)\n",
    "---------------------------------\n",
    "\n",
    "There are various ways to evaluate the performance of link prediction.\n",
    "This tutorial follows the practice of `GraphSAGE\n",
    "paper <https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf>`__,\n",
    "where it treats the node embeddings learned by link prediction via\n",
    "training and evaluating a linear classifier on top of the learned node\n",
    "embeddings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the representations of all the nodes, this tutorial uses\n",
    "neighbor sampling as introduced in the :doc:`node classification\n",
    "tutorial <L1_large_node_classification>`.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>If you would like to obtain node representations without\n",
    "   neighbor sampling during inference, please refer to this `user\n",
    "   guide <guide-minibatch-inference>`.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, node_features):\n",
    "    with torch.no_grad():\n",
    "        nodes = torch.arange(graph.number_of_nodes())\n",
    "\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "        train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            graph, torch.arange(graph.number_of_nodes()), sampler,\n",
    "            batch_size=1024,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=4,\n",
    "            device=device)\n",
    "\n",
    "        result = []\n",
    "        for input_nodes, output_nodes, bipartites in train_dataloader:\n",
    "            # feature copy from CPU to GPU takes place here\n",
    "            inputs = bipartites[0].srcdata['feat']\n",
    "            result.append(model(bipartites, inputs))\n",
    "\n",
    "        return torch.cat(result)\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "def evaluate(emb, label, train_nids, valid_nids, test_nids):\n",
    "    classifier = nn.Linear(emb.shape[1], num_classes).to(device)\n",
    "    opt = torch.optim.LBFGS(classifier.parameters())\n",
    "\n",
    "    def compute_loss():\n",
    "        pred = classifier(emb[train_nids].to(device))\n",
    "        loss = F.cross_entropy(pred, label[train_nids].to(device))\n",
    "        return loss\n",
    "\n",
    "    def closure():\n",
    "        loss = compute_loss()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    prev_loss = float('inf')\n",
    "    for i in range(1000):\n",
    "        opt.step(closure)\n",
    "        with torch.no_grad():\n",
    "            loss = compute_loss().item()\n",
    "            if np.abs(loss - prev_loss) < 1e-4:\n",
    "                print('Converges at iteration', i)\n",
    "                break\n",
    "            else:\n",
    "                prev_loss = loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = classifier(emb.to(device)).cpu()\n",
    "        label = label\n",
    "        valid_acc = sklearn.metrics.accuracy_score(label[valid_nids].numpy(), pred[valid_nids].numpy().argmax(1))\n",
    "        test_acc = sklearn.metrics.accuracy_score(label[test_nids].numpy(), pred[test_nids].numpy().argmax(1))\n",
    "    return valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Training Loop\n",
    "----------------------\n",
    "\n",
    "The following initializes the model and defines the optimizer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(node_features.shape[1], 128).to(device)\n",
    "predictor = DotPredictor().to(device)\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the training loop for link prediction and\n",
    "evaluation, and also saves the model that performs the best on the\n",
    "validation set:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 499/2278 [00:33<02:00, 14.81it/s, loss=0.645]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-bdc154ed11b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_nids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_nids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {} Validation Accuracy {} Test Accuracy {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-330386e23b00>\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, graph, node_features)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiLayerNeighborSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         train_dataloader = dgl.dataloading.NodeDataLoader(\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dgl/lib/python3.8/site-packages/dgl/dataloading/pytorch/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, g, nids, block_sampler, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_NodeCollator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcollator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             self.dataloader = DataLoader(self.collator.dataset,\n\u001b[0m\u001b[1;32m    218\u001b[0m                                          \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                          **dataloader_kwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(1):\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, pos_graph, neg_graph, bipartites) in enumerate(tq):\n",
    "            # feature copy from CPU to GPU takes place here\n",
    "            inputs = bipartites[0].srcdata['feat']\n",
    "\n",
    "            outputs = model(bipartites, inputs)\n",
    "            pos_score = predictor(pos_graph, outputs)\n",
    "            neg_score = predictor(neg_graph, outputs)\n",
    "\n",
    "            score = torch.cat([pos_score, neg_score])\n",
    "            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "\n",
    "            if (step + 1) % 500 == 0:\n",
    "                model.eval()\n",
    "                emb = inference(model, graph, node_features)\n",
    "                valid_acc, test_acc = evaluate(emb, node_labels, train_nids, valid_nids, test_nids)\n",
    "                print('Epoch {} Validation Accuracy {} Test Accuracy {}'.format(epoch, valid_acc, test_acc))\n",
    "                if best_accuracy < valid_acc:\n",
    "                    best_accuracy = valid_acc\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                model.train()\n",
    "\n",
    "                # Note that this tutorial do not train the whole model to the end.\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE\n",
    "for link prediction with neighbor sampling.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
